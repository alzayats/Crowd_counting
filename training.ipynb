{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\anacond33\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "from model import CSRNet\n",
    "\n",
    "from utils import save_checkpoint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import cv2\n",
    "import dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_list, model, criterion, optimizer, epoch):\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset.listDataset(train_list,\n",
    "                       shuffle=True,\n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                   ]), \n",
    "                       train=True, \n",
    "                       seen=model.seen,\n",
    "                       batch_size=batch_size,\n",
    "                       num_workers=workers),\n",
    "        batch_size=batch_size)\n",
    "    print('epoch %d, processed %d samples, lr %.10f' % (epoch, epoch * len(train_loader.dataset), lr))\n",
    "    \n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    \n",
    "    for i,(img, target)in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        img = img.cuda()\n",
    "        img = Variable(img)\n",
    "        output = model(img)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        target = target.type(torch.FloatTensor).unsqueeze(0).cuda()\n",
    "        target = Variable(target)\n",
    "        \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        losses.update(loss.item(), img.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  .format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(val_list, model, criterion):\n",
    "    print ('begin test')\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "    dataset.listDataset(val_list,\n",
    "                   shuffle=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                   ]),  train=False),\n",
    "    batch_size=batch_size)    \n",
    "    \n",
    "    #model.eval()\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        mae = 0\n",
    "        \n",
    "        for i,(img, target) in enumerate(test_loader):\n",
    "            img = img.cuda()\n",
    "            img = Variable(img)\n",
    "            output = model(img)\n",
    "            \n",
    "            mae += abs(output.data.sum()-target.sum().type(torch.FloatTensor).cuda())\n",
    "            \n",
    "        mae = mae/len(test_loader)    \n",
    "        print(' * MAE {mae:.3f} '\n",
    "                  .format(mae=mae))\n",
    "\n",
    "    return mae    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    \n",
    "    \n",
    "    lr = original_lr\n",
    "    \n",
    "    for i in range(len(steps)):\n",
    "        \n",
    "        scale = scales[i] if i < len(scales) else 1\n",
    "        \n",
    "        \n",
    "        if epoch >= steps[i]:\n",
    "            lr = lr * scale\n",
    "            if epoch == steps[i]:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_prec1    = 1e6\n",
    "original_lr    = 1e-7\n",
    "lr = 1e-7\n",
    "batch_size    = 1\n",
    "momentum      = 0.95\n",
    "decay         = 5*1e-4\n",
    "start_epoch   = 0\n",
    "epochs = 4\n",
    "steps         = [-1,1,100,150]\n",
    "scales        = [1,1,1,1]\n",
    "workers = 4\n",
    "seed = time.time()\n",
    "print_freq = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_json, test_json, pre,gpu, task):\n",
    "    \n",
    "    global best_prec1\n",
    "    \n",
    "    with open(train_json, 'r') as outfile:        \n",
    "        train_list = json.load(outfile)\n",
    "    with open(test_json, 'r') as outfile:       \n",
    "        val_list = json.load(outfile)\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    model = CSRNet()\n",
    "    \n",
    "    model = model.cuda()\n",
    "    \n",
    "    criterion = nn.MSELoss(size_average=False).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=decay)\n",
    "\n",
    "#     if pre != '0':\n",
    "#         if os.path.isfile(pre):\n",
    "#             print(\"=> loading checkpoint '{}'\".format(pre))\n",
    "#             checkpoint = torch.load(pre)\n",
    "#             start_epoch = checkpoint['epoch']\n",
    "#             best_prec1 = checkpoint['best_prec1']\n",
    "#             model.load_state_dict(checkpoint['state_dict'])\n",
    "#             optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#             print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "#                   .format(pre, checkpoint['epoch']))\n",
    "#         else:\n",
    "#             print(\"=> no checkpoint found at '{}'\".format(pre))\n",
    "            \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        train(train_list, model, criterion, optimizer, epoch)\n",
    "        prec1 = validate(val_list, model, criterion)\n",
    "        \n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        print(' * best MAE {mae:.3f} '\n",
    "              .format(mae=best_prec1))\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': pre,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best,task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\anacond33\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, processed 0 samples, lr 0.0000001000\n",
      "Epoch: [0][0/1928]\tTime 1.220 (1.220)\tData 0.082 (0.082)\tLoss 339.6719 (339.6719)\t\n",
      "Epoch: [0][30/1928]\tTime 1.121 (1.056)\tData 0.051 (0.081)\tLoss 74.6359 (511.1239)\t\n",
      "Epoch: [0][60/1928]\tTime 0.848 (0.965)\tData 0.013 (0.059)\tLoss 522.4189 (425.4228)\t\n",
      "Epoch: [0][90/1928]\tTime 0.607 (1.032)\tData 0.044 (0.055)\tLoss 57.6476 (391.9592)\t\n",
      "Epoch: [0][120/1928]\tTime 1.196 (1.032)\tData 0.046 (0.052)\tLoss 3407.2883 (395.8359)\t\n",
      "Epoch: [0][150/1928]\tTime 1.059 (1.045)\tData 0.015 (0.050)\tLoss 21.8681 (380.3613)\t\n",
      "Epoch: [0][180/1928]\tTime 0.892 (1.039)\tData 0.025 (0.052)\tLoss 17.0262 (378.5682)\t\n",
      "Epoch: [0][210/1928]\tTime 1.186 (1.025)\tData 0.041 (0.050)\tLoss 414.2641 (361.9537)\t\n",
      "Epoch: [0][240/1928]\tTime 0.535 (1.026)\tData 0.037 (0.048)\tLoss 130.3495 (370.6297)\t\n",
      "Epoch: [0][270/1928]\tTime 1.691 (1.037)\tData 0.075 (0.048)\tLoss 78.1689 (356.0024)\t\n",
      "Epoch: [0][300/1928]\tTime 1.472 (1.037)\tData 0.046 (0.047)\tLoss 272.8267 (373.4571)\t\n",
      "Epoch: [0][330/1928]\tTime 0.596 (1.040)\tData 0.048 (0.047)\tLoss 55.6531 (387.8511)\t\n",
      "Epoch: [0][360/1928]\tTime 1.537 (1.065)\tData 0.059 (0.046)\tLoss 244.7166 (379.9656)\t\n",
      "Epoch: [0][390/1928]\tTime 1.336 (1.080)\tData 0.071 (0.047)\tLoss 782.4971 (394.3999)\t\n",
      "Epoch: [0][420/1928]\tTime 0.784 (1.082)\tData 0.016 (0.046)\tLoss 38.8290 (397.7185)\t\n",
      "Epoch: [0][450/1928]\tTime 1.695 (1.098)\tData 0.067 (0.046)\tLoss 135.7051 (387.6261)\t\n",
      "Epoch: [0][480/1928]\tTime 1.583 (1.092)\tData 0.017 (0.046)\tLoss 108.7898 (387.7656)\t\n",
      "Epoch: [0][510/1928]\tTime 1.342 (1.105)\tData 0.036 (0.045)\tLoss 5566.3071 (392.0678)\t\n",
      "Epoch: [0][540/1928]\tTime 2.265 (1.106)\tData 0.052 (0.045)\tLoss 57.6382 (397.1070)\t\n",
      "Epoch: [0][570/1928]\tTime 0.827 (1.116)\tData 0.049 (0.045)\tLoss 88.7456 (399.0303)\t\n",
      "Epoch: [0][600/1928]\tTime 1.345 (1.128)\tData 0.051 (0.044)\tLoss 27.6517 (401.5739)\t\n",
      "Epoch: [0][630/1928]\tTime 0.467 (1.124)\tData 0.011 (0.044)\tLoss 29.7608 (397.9927)\t\n",
      "Epoch: [0][660/1928]\tTime 1.163 (1.143)\tData 0.048 (0.044)\tLoss 21.1074 (387.0029)\t\n",
      "Epoch: [0][690/1928]\tTime 1.030 (1.151)\tData 0.046 (0.044)\tLoss 459.5898 (379.3185)\t\n",
      "Epoch: [0][720/1928]\tTime 1.542 (1.157)\tData 0.053 (0.044)\tLoss 177.0556 (385.3290)\t\n",
      "Epoch: [0][750/1928]\tTime 1.159 (1.174)\tData 0.052 (0.044)\tLoss 533.7529 (380.4833)\t\n",
      "Epoch: [0][780/1928]\tTime 2.146 (1.178)\tData 0.050 (0.044)\tLoss 19.4881 (374.1930)\t\n",
      "Epoch: [0][810/1928]\tTime 0.708 (1.178)\tData 0.054 (0.044)\tLoss 24.3625 (367.6166)\t\n",
      "Epoch: [0][840/1928]\tTime 1.942 (1.188)\tData 0.034 (0.044)\tLoss 12.9712 (364.9158)\t\n",
      "Epoch: [0][870/1928]\tTime 1.259 (1.185)\tData 0.036 (0.044)\tLoss 435.4959 (359.3124)\t\n",
      "Epoch: [0][900/1928]\tTime 2.290 (1.198)\tData 0.038 (0.044)\tLoss 148.7386 (358.7645)\t\n",
      "Epoch: [0][930/1928]\tTime 1.497 (1.199)\tData 0.047 (0.044)\tLoss 73.5824 (359.1520)\t\n",
      "Epoch: [0][960/1928]\tTime 0.893 (1.210)\tData 0.050 (0.043)\tLoss 2783.0691 (360.5651)\t\n",
      "Epoch: [0][990/1928]\tTime 1.356 (1.214)\tData 0.030 (0.043)\tLoss 66.5983 (362.6402)\t\n",
      "Epoch: [0][1020/1928]\tTime 1.869 (1.224)\tData 0.043 (0.043)\tLoss 78.3284 (357.9682)\t\n",
      "Epoch: [0][1050/1928]\tTime 1.629 (1.231)\tData 0.051 (0.043)\tLoss 254.6691 (352.6413)\t\n",
      "Epoch: [0][1080/1928]\tTime 2.082 (1.230)\tData 0.051 (0.043)\tLoss 23.6208 (346.8349)\t\n",
      "Epoch: [0][1110/1928]\tTime 1.743 (1.245)\tData 0.058 (0.043)\tLoss 109.5143 (351.3605)\t\n",
      "Epoch: [0][1140/1928]\tTime 1.910 (1.256)\tData 0.049 (0.043)\tLoss 86.4498 (350.9587)\t\n",
      "Epoch: [0][1170/1928]\tTime 1.231 (1.259)\tData 0.036 (0.043)\tLoss 68.8288 (352.9024)\t\n",
      "Epoch: [0][1200/1928]\tTime 1.921 (1.263)\tData 0.050 (0.043)\tLoss 41.9500 (347.7938)\t\n",
      "Epoch: [0][1230/1928]\tTime 2.419 (1.272)\tData 0.071 (0.043)\tLoss 126.1009 (344.7079)\t\n",
      "Epoch: [0][1260/1928]\tTime 0.515 (1.281)\tData 0.012 (0.043)\tLoss 3.6395 (340.9172)\t\n",
      "Epoch: [0][1290/1928]\tTime 1.407 (1.289)\tData 0.055 (0.043)\tLoss 95.2142 (346.5309)\t\n",
      "Epoch: [0][1320/1928]\tTime 2.072 (1.296)\tData 0.050 (0.042)\tLoss 24.6144 (346.9338)\t\n",
      "Epoch: [0][1350/1928]\tTime 2.051 (1.304)\tData 0.047 (0.042)\tLoss 30.0062 (346.7331)\t\n",
      "Epoch: [0][1380/1928]\tTime 2.215 (1.309)\tData 0.024 (0.042)\tLoss 1274.8273 (344.9279)\t\n",
      "Epoch: [0][1410/1928]\tTime 0.965 (1.310)\tData 0.030 (0.042)\tLoss 2083.7034 (343.8727)\t\n",
      "Epoch: [0][1440/1928]\tTime 1.215 (1.313)\tData 0.048 (0.042)\tLoss 63.7044 (340.7952)\t\n",
      "Epoch: [0][1470/1928]\tTime 0.804 (1.315)\tData 0.017 (0.042)\tLoss 166.8266 (344.7787)\t\n",
      "Epoch: [0][1500/1928]\tTime 2.074 (1.318)\tData 0.058 (0.042)\tLoss 2046.6177 (346.6747)\t\n",
      "Epoch: [0][1530/1928]\tTime 1.571 (1.326)\tData 0.036 (0.042)\tLoss 429.6711 (346.5132)\t\n",
      "Epoch: [0][1560/1928]\tTime 1.437 (1.333)\tData 0.053 (0.042)\tLoss 21.4087 (345.1611)\t\n",
      "Epoch: [0][1590/1928]\tTime 1.876 (1.339)\tData 0.019 (0.042)\tLoss 49.9492 (341.8077)\t\n",
      "Epoch: [0][1620/1928]\tTime 1.894 (1.343)\tData 0.041 (0.042)\tLoss 151.7380 (339.9836)\t\n",
      "Epoch: [0][1650/1928]\tTime 0.731 (1.346)\tData 0.021 (0.042)\tLoss 534.8572 (339.5531)\t\n",
      "Epoch: [0][1680/1928]\tTime 1.257 (1.351)\tData 0.054 (0.041)\tLoss 2051.7244 (342.5741)\t\n",
      "Epoch: [0][1710/1928]\tTime 2.069 (1.360)\tData 0.053 (0.042)\tLoss 30.1994 (344.4130)\t\n",
      "Epoch: [0][1740/1928]\tTime 2.182 (1.370)\tData 0.050 (0.042)\tLoss 12.5405 (342.7444)\t\n",
      "Epoch: [0][1770/1928]\tTime 2.227 (1.372)\tData 0.050 (0.042)\tLoss 249.8040 (338.7857)\t\n",
      "Epoch: [0][1800/1928]\tTime 2.305 (1.377)\tData 0.045 (0.042)\tLoss 20.0465 (336.9916)\t\n",
      "Epoch: [0][1830/1928]\tTime 2.077 (1.381)\tData 0.050 (0.042)\tLoss 213.8820 (339.5644)\t\n",
      "Epoch: [0][1860/1928]\tTime 2.130 (1.390)\tData 0.048 (0.042)\tLoss 20.6774 (336.7432)\t\n",
      "Epoch: [0][1890/1928]\tTime 1.221 (1.390)\tData 0.051 (0.042)\tLoss 942.5124 (335.2628)\t\n",
      "Epoch: [0][1920/1928]\tTime 2.075 (1.397)\tData 0.052 (0.042)\tLoss 167.0835 (334.0881)\t\n",
      "begin test\n",
      " * MAE 241.731 \n",
      " * best MAE 241.731 \n",
      "epoch 1, processed 1928 samples, lr 0.0000001000\n",
      "Epoch: [1][0/1928]\tTime 0.387 (0.387)\tData 0.040 (0.040)\tLoss 24.8816 (24.8816)\t\n",
      "Epoch: [1][30/1928]\tTime 1.532 (1.496)\tData 0.013 (0.040)\tLoss 5.5195 (383.8345)\t\n",
      "Epoch: [1][60/1928]\tTime 1.076 (1.438)\tData 0.041 (0.038)\tLoss 18.8832 (433.5607)\t\n",
      "Epoch: [1][90/1928]\tTime 2.061 (1.497)\tData 0.047 (0.039)\tLoss 23.9070 (364.1138)\t\n",
      "Epoch: [1][120/1928]\tTime 1.880 (1.517)\tData 0.040 (0.039)\tLoss 249.7552 (362.1975)\t\n",
      "Epoch: [1][150/1928]\tTime 2.077 (1.534)\tData 0.050 (0.039)\tLoss 20.9801 (335.9726)\t\n",
      "Epoch: [1][180/1928]\tTime 1.157 (1.518)\tData 0.019 (0.038)\tLoss 91.7155 (360.8819)\t\n",
      "Epoch: [1][210/1928]\tTime 0.691 (1.534)\tData 0.035 (0.039)\tLoss 318.3712 (343.2558)\t\n",
      "Epoch: [1][240/1928]\tTime 2.259 (1.568)\tData 0.053 (0.040)\tLoss 170.9747 (378.3837)\t\n",
      "Epoch: [1][270/1928]\tTime 1.437 (1.546)\tData 0.058 (0.040)\tLoss 360.9165 (370.4192)\t\n",
      "Epoch: [1][300/1928]\tTime 0.561 (1.519)\tData 0.017 (0.040)\tLoss 8.6298 (357.3752)\t\n",
      "Epoch: [1][330/1928]\tTime 0.719 (1.506)\tData 0.039 (0.040)\tLoss 28.1778 (373.2789)\t\n",
      "Epoch: [1][360/1928]\tTime 1.490 (1.462)\tData 0.057 (0.040)\tLoss 1643.2053 (361.8261)\t\n",
      "Epoch: [1][390/1928]\tTime 1.086 (1.458)\tData 0.047 (0.040)\tLoss 39.9500 (378.1802)\t\n",
      "Epoch: [1][420/1928]\tTime 1.169 (1.432)\tData 0.024 (0.040)\tLoss 16.0697 (390.1487)\t\n",
      "Epoch: [1][450/1928]\tTime 2.118 (1.431)\tData 0.053 (0.040)\tLoss 146.4813 (377.4613)\t\n",
      "Epoch: [1][480/1928]\tTime 1.183 (1.411)\tData 1.007 (0.042)\tLoss 44.1489 (373.6537)\t\n",
      "Epoch: [1][510/1928]\tTime 0.939 (1.388)\tData 0.025 (0.042)\tLoss 328.1438 (364.2604)\t\n",
      "Epoch: [1][540/1928]\tTime 1.707 (1.372)\tData 0.013 (0.042)\tLoss 387.9597 (357.9757)\t\n",
      "Epoch: [1][570/1928]\tTime 0.356 (1.361)\tData 0.018 (0.041)\tLoss 605.3165 (347.9396)\t\n",
      "Epoch: [1][600/1928]\tTime 1.116 (1.355)\tData 0.054 (0.042)\tLoss 891.1493 (351.2020)\t\n",
      "Epoch: [1][630/1928]\tTime 1.561 (1.354)\tData 0.059 (0.041)\tLoss 23.2906 (343.9156)\t\n",
      "Epoch: [1][660/1928]\tTime 1.435 (1.344)\tData 0.054 (0.041)\tLoss 94.4042 (346.5652)\t\n",
      "Epoch: [1][690/1928]\tTime 2.332 (1.346)\tData 0.058 (0.042)\tLoss 751.0104 (337.6470)\t\n",
      "Epoch: [1][720/1928]\tTime 0.689 (1.335)\tData 0.048 (0.041)\tLoss 20.4441 (332.4030)\t\n",
      "Epoch: [1][750/1928]\tTime 1.147 (1.331)\tData 0.051 (0.041)\tLoss 677.6077 (330.6753)\t\n",
      "Epoch: [1][780/1928]\tTime 1.182 (1.327)\tData 0.049 (0.041)\tLoss 146.5303 (327.2829)\t\n",
      "Epoch: [1][810/1928]\tTime 1.743 (1.322)\tData 0.025 (0.041)\tLoss 470.7102 (322.2692)\t\n",
      "Epoch: [1][840/1928]\tTime 1.305 (1.320)\tData 0.061 (0.041)\tLoss 31.1111 (318.4800)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][870/1928]\tTime 1.164 (1.310)\tData 0.015 (0.041)\tLoss 30.3526 (313.3175)\t\n",
      "Epoch: [1][900/1928]\tTime 0.819 (1.309)\tData 0.050 (0.041)\tLoss 488.0224 (316.0266)\t\n",
      "Epoch: [1][930/1928]\tTime 1.039 (1.307)\tData 0.079 (0.041)\tLoss 4868.5723 (323.0330)\t\n",
      "Epoch: [1][960/1928]\tTime 1.048 (1.305)\tData 0.052 (0.041)\tLoss 334.1432 (325.5874)\t\n",
      "Epoch: [1][990/1928]\tTime 1.539 (1.298)\tData 0.047 (0.041)\tLoss 26.5695 (328.7887)\t\n",
      "Epoch: [1][1020/1928]\tTime 1.073 (1.301)\tData 0.050 (0.041)\tLoss 292.6115 (324.2220)\t\n",
      "Epoch: [1][1050/1928]\tTime 1.102 (1.295)\tData 0.016 (0.041)\tLoss 29.0900 (323.2706)\t\n",
      "Epoch: [1][1080/1928]\tTime 1.494 (1.298)\tData 0.050 (0.041)\tLoss 396.4064 (323.0063)\t\n",
      "Epoch: [1][1110/1928]\tTime 1.439 (1.293)\tData 0.046 (0.041)\tLoss 94.8621 (329.0997)\t\n",
      "Epoch: [1][1140/1928]\tTime 0.820 (1.294)\tData 0.052 (0.041)\tLoss 299.4645 (325.6952)\t\n",
      "Epoch: [1][1170/1928]\tTime 1.199 (1.289)\tData 0.016 (0.041)\tLoss 59.6647 (323.2660)\t\n",
      "Epoch: [1][1200/1928]\tTime 1.102 (1.292)\tData 0.017 (0.041)\tLoss 25.3144 (325.6598)\t\n",
      "Epoch: [1][1230/1928]\tTime 2.017 (1.288)\tData 0.050 (0.041)\tLoss 7.0263 (322.1292)\t\n",
      "Epoch: [1][1260/1928]\tTime 1.501 (1.289)\tData 0.052 (0.041)\tLoss 58.9203 (317.3804)\t\n",
      "Epoch: [1][1290/1928]\tTime 1.257 (1.283)\tData 0.029 (0.041)\tLoss 50.0831 (316.4722)\t\n",
      "Epoch: [1][1320/1928]\tTime 0.922 (1.283)\tData 0.065 (0.041)\tLoss 109.9154 (318.2008)\t\n",
      "Epoch: [1][1350/1928]\tTime 2.150 (1.279)\tData 0.058 (0.041)\tLoss 162.7033 (323.7472)\t\n",
      "Epoch: [1][1380/1928]\tTime 1.166 (1.280)\tData 0.024 (0.041)\tLoss 14.1900 (321.0004)\t\n",
      "Epoch: [1][1410/1928]\tTime 2.008 (1.282)\tData 0.035 (0.041)\tLoss 534.8495 (319.1778)\t\n",
      "Epoch: [1][1440/1928]\tTime 0.771 (1.280)\tData 0.055 (0.041)\tLoss 41.6353 (317.8454)\t\n",
      "Epoch: [1][1470/1928]\tTime 0.930 (1.280)\tData 0.016 (0.041)\tLoss 275.0241 (315.2401)\t\n",
      "Epoch: [1][1500/1928]\tTime 1.142 (1.278)\tData 0.022 (0.041)\tLoss 65.5303 (315.0921)\t\n",
      "Epoch: [1][1530/1928]\tTime 2.150 (1.281)\tData 0.055 (0.041)\tLoss 71.2486 (313.1449)\t\n",
      "Epoch: [1][1560/1928]\tTime 0.678 (1.279)\tData 0.029 (0.041)\tLoss 1787.1982 (316.7944)\t\n",
      "Epoch: [1][1590/1928]\tTime 1.486 (1.282)\tData 0.047 (0.041)\tLoss 688.1863 (315.4904)\t\n",
      "Epoch: [1][1620/1928]\tTime 1.167 (1.279)\tData 0.025 (0.041)\tLoss 991.6113 (314.6229)\t\n",
      "Epoch: [1][1650/1928]\tTime 1.482 (1.280)\tData 0.050 (0.041)\tLoss 19.3385 (310.6368)\t\n",
      "Epoch: [1][1680/1928]\tTime 1.706 (1.275)\tData 0.053 (0.041)\tLoss 21.3500 (309.3228)\t\n",
      "Epoch: [1][1710/1928]\tTime 1.188 (1.277)\tData 0.027 (0.041)\tLoss 119.4737 (307.4597)\t\n",
      "Epoch: [1][1740/1928]\tTime 2.077 (1.277)\tData 0.047 (0.041)\tLoss 70.2055 (309.9453)\t\n",
      "Epoch: [1][1770/1928]\tTime 1.370 (1.277)\tData 0.040 (0.041)\tLoss 380.4396 (308.7197)\t\n",
      "Epoch: [1][1800/1928]\tTime 1.518 (1.280)\tData 0.055 (0.041)\tLoss 95.7276 (313.6244)\t\n",
      "Epoch: [1][1830/1928]\tTime 1.826 (1.275)\tData 0.050 (0.041)\tLoss 209.5101 (312.6612)\t\n",
      "Epoch: [1][1860/1928]\tTime 0.898 (1.278)\tData 0.040 (0.041)\tLoss 43.1540 (312.5909)\t\n",
      "Epoch: [1][1890/1928]\tTime 2.255 (1.276)\tData 0.026 (0.041)\tLoss 42.3137 (311.9756)\t\n",
      "Epoch: [1][1920/1928]\tTime 1.491 (1.277)\tData 0.053 (0.041)\tLoss 124.5907 (313.4870)\t\n",
      "begin test\n",
      " * MAE 259.632 \n",
      " * best MAE 241.731 \n",
      "epoch 2, processed 3856 samples, lr 0.0000001000\n",
      "Epoch: [2][0/1928]\tTime 0.531 (0.531)\tData 0.060 (0.060)\tLoss 22.2300 (22.2300)\t\n",
      "Epoch: [2][30/1928]\tTime 1.385 (1.562)\tData 0.051 (0.044)\tLoss 187.1064 (219.0669)\t\n",
      "Epoch: [2][60/1928]\tTime 0.652 (1.389)\tData 0.023 (0.041)\tLoss 2.4940 (210.0210)\t\n",
      "Epoch: [2][90/1928]\tTime 1.239 (1.385)\tData 0.031 (0.042)\tLoss 55.0719 (272.5247)\t\n",
      "Epoch: [2][120/1928]\tTime 0.180 (1.369)\tData 0.012 (0.040)\tLoss 462.3918 (282.1850)\t\n",
      "Epoch: [2][150/1928]\tTime 1.078 (1.302)\tData 0.045 (0.040)\tLoss 11.6016 (265.1831)\t\n",
      "Epoch: [2][180/1928]\tTime 0.653 (1.330)\tData 0.054 (0.040)\tLoss 435.8227 (287.1122)\t\n",
      "Epoch: [2][210/1928]\tTime 1.526 (1.299)\tData 0.010 (0.039)\tLoss 51.2464 (286.6963)\t\n",
      "Epoch: [2][240/1928]\tTime 0.254 (1.298)\tData 0.015 (0.039)\tLoss 33.4754 (297.5686)\t\n",
      "Epoch: [2][270/1928]\tTime 0.706 (1.263)\tData 0.030 (0.038)\tLoss 72.6791 (286.0390)\t\n",
      "Epoch: [2][300/1928]\tTime 1.228 (1.274)\tData 0.049 (0.038)\tLoss 191.2121 (281.8901)\t\n",
      "Epoch: [2][330/1928]\tTime 0.924 (1.258)\tData 0.025 (0.038)\tLoss 44.6693 (277.9196)\t\n",
      "Epoch: [2][360/1928]\tTime 0.750 (1.265)\tData 0.046 (0.038)\tLoss 20.1274 (302.9600)\t\n",
      "Epoch: [2][390/1928]\tTime 1.747 (1.277)\tData 0.050 (0.039)\tLoss 59.9242 (303.6268)\t\n",
      "Epoch: [2][420/1928]\tTime 1.000 (1.273)\tData 0.062 (0.039)\tLoss 12.3774 (293.1365)\t\n",
      "Epoch: [2][450/1928]\tTime 1.949 (1.275)\tData 0.040 (0.039)\tLoss 340.2072 (283.7424)\t\n",
      "Epoch: [2][480/1928]\tTime 1.389 (1.264)\tData 0.049 (0.039)\tLoss 41.9713 (275.9334)\t\n",
      "Epoch: [2][510/1928]\tTime 1.450 (1.279)\tData 0.110 (0.039)\tLoss 330.7576 (276.5779)\t\n",
      "Epoch: [2][540/1928]\tTime 2.252 (1.277)\tData 0.052 (0.039)\tLoss 208.9068 (272.5201)\t\n",
      "Epoch: [2][570/1928]\tTime 0.846 (1.280)\tData 0.052 (0.039)\tLoss 16.5051 (265.6896)\t\n",
      "Epoch: [2][600/1928]\tTime 2.050 (1.291)\tData 0.048 (0.039)\tLoss 56.6938 (262.4867)\t\n",
      "Epoch: [2][630/1928]\tTime 0.816 (1.288)\tData 0.024 (0.039)\tLoss 14.4019 (264.7676)\t\n",
      "Epoch: [2][660/1928]\tTime 1.389 (1.288)\tData 0.047 (0.039)\tLoss 416.3102 (268.2081)\t\n",
      "Epoch: [2][690/1928]\tTime 1.137 (1.300)\tData 0.019 (0.040)\tLoss 14.6545 (260.9525)\t\n",
      "Epoch: [2][720/1928]\tTime 0.688 (1.303)\tData 0.013 (0.040)\tLoss 6.8198 (262.9756)\t\n",
      "Epoch: [2][750/1928]\tTime 1.106 (1.303)\tData 0.022 (0.040)\tLoss 66.7917 (269.2695)\t\n",
      "Epoch: [2][780/1928]\tTime 1.652 (1.310)\tData 0.019 (0.040)\tLoss 74.2080 (268.1130)\t\n",
      "Epoch: [2][810/1928]\tTime 0.481 (1.301)\tData 0.019 (0.040)\tLoss 225.9753 (271.6858)\t\n",
      "Epoch: [2][840/1928]\tTime 1.189 (1.312)\tData 0.071 (0.040)\tLoss 78.1760 (273.7351)\t\n",
      "Epoch: [2][870/1928]\tTime 2.249 (1.316)\tData 0.047 (0.040)\tLoss 23.6909 (272.2842)\t\n",
      "Epoch: [2][900/1928]\tTime 1.911 (1.312)\tData 0.052 (0.040)\tLoss 88.7432 (269.1240)\t\n",
      "Epoch: [2][930/1928]\tTime 1.428 (1.319)\tData 0.051 (0.040)\tLoss 100.8656 (278.5120)\t\n",
      "Epoch: [2][960/1928]\tTime 2.053 (1.316)\tData 0.048 (0.040)\tLoss 984.9106 (281.1600)\t\n",
      "Epoch: [2][990/1928]\tTime 1.485 (1.317)\tData 0.055 (0.040)\tLoss 387.7458 (280.7816)\t\n",
      "Epoch: [2][1020/1928]\tTime 1.708 (1.315)\tData 0.049 (0.040)\tLoss 22.0066 (281.9519)\t\n",
      "Epoch: [2][1050/1928]\tTime 0.509 (1.312)\tData 0.021 (0.040)\tLoss 101.6091 (282.5226)\t\n",
      "Epoch: [2][1080/1928]\tTime 1.251 (1.317)\tData 0.034 (0.040)\tLoss 9.7626 (282.3170)\t\n",
      "Epoch: [2][1110/1928]\tTime 1.797 (1.317)\tData 0.051 (0.040)\tLoss 186.1888 (279.5704)\t\n",
      "Epoch: [2][1140/1928]\tTime 1.363 (1.320)\tData 0.044 (0.040)\tLoss 1777.7914 (285.1386)\t\n",
      "Epoch: [2][1170/1928]\tTime 1.437 (1.316)\tData 0.021 (0.040)\tLoss 186.0475 (288.5947)\t\n",
      "Epoch: [2][1200/1928]\tTime 1.383 (1.313)\tData 0.052 (0.040)\tLoss 272.2769 (294.3120)\t\n",
      "Epoch: [2][1230/1928]\tTime 1.022 (1.312)\tData 0.059 (0.040)\tLoss 71.7070 (295.1996)\t\n",
      "Epoch: [2][1260/1928]\tTime 0.716 (1.310)\tData 0.043 (0.040)\tLoss 121.1232 (295.1612)\t\n",
      "Epoch: [2][1290/1928]\tTime 1.434 (1.313)\tData 0.051 (0.040)\tLoss 233.8276 (294.3493)\t\n",
      "Epoch: [2][1320/1928]\tTime 2.331 (1.311)\tData 0.057 (0.040)\tLoss 5.3515 (295.1001)\t\n",
      "Epoch: [2][1350/1928]\tTime 0.643 (1.310)\tData 0.054 (0.040)\tLoss 775.6274 (298.3046)\t\n",
      "Epoch: [2][1380/1928]\tTime 1.677 (1.314)\tData 0.022 (0.041)\tLoss 20.5292 (299.1540)\t\n",
      "Epoch: [2][1410/1928]\tTime 0.596 (1.313)\tData 0.012 (0.041)\tLoss 423.5289 (299.3739)\t\n",
      "Epoch: [2][1440/1928]\tTime 1.320 (1.311)\tData 0.044 (0.041)\tLoss 36.2917 (295.9982)\t\n",
      "Epoch: [2][1470/1928]\tTime 1.488 (1.309)\tData 0.050 (0.041)\tLoss 135.2858 (296.5186)\t\n",
      "Epoch: [2][1500/1928]\tTime 1.178 (1.316)\tData 0.025 (0.041)\tLoss 35.6015 (295.2135)\t\n",
      "Epoch: [2][1530/1928]\tTime 1.208 (1.310)\tData 0.019 (0.041)\tLoss 120.9858 (297.9869)\t\n",
      "Epoch: [2][1560/1928]\tTime 1.209 (1.311)\tData 0.041 (0.041)\tLoss 288.2357 (296.7037)\t\n",
      "Epoch: [2][1590/1928]\tTime 2.251 (1.308)\tData 0.064 (0.040)\tLoss 209.8017 (294.6581)\t\n",
      "Epoch: [2][1620/1928]\tTime 1.220 (1.310)\tData 0.018 (0.040)\tLoss 65.3558 (291.6339)\t\n",
      "Epoch: [2][1650/1928]\tTime 1.378 (1.310)\tData 0.057 (0.040)\tLoss 136.0192 (290.1548)\t\n",
      "Epoch: [2][1680/1928]\tTime 1.345 (1.308)\tData 0.049 (0.040)\tLoss 133.6701 (288.9236)\t\n",
      "Epoch: [2][1710/1928]\tTime 0.667 (1.313)\tData 0.029 (0.040)\tLoss 46.0712 (289.1430)\t\n",
      "Epoch: [2][1740/1928]\tTime 1.217 (1.311)\tData 0.022 (0.041)\tLoss 438.5303 (293.1633)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1770/1928]\tTime 1.254 (1.312)\tData 0.031 (0.040)\tLoss 1715.2294 (293.9072)\t\n",
      "Epoch: [2][1800/1928]\tTime 1.822 (1.314)\tData 0.035 (0.040)\tLoss 118.1933 (294.6539)\t\n",
      "Epoch: [2][1830/1928]\tTime 0.966 (1.312)\tData 0.075 (0.041)\tLoss 81.0025 (295.1875)\t\n",
      "Epoch: [2][1860/1928]\tTime 0.946 (1.317)\tData 0.040 (0.041)\tLoss 1564.6898 (296.1458)\t\n",
      "Epoch: [2][1890/1928]\tTime 1.691 (1.317)\tData 0.051 (0.041)\tLoss 78.3446 (299.5637)\t\n",
      "Epoch: [2][1920/1928]\tTime 1.180 (1.318)\tData 0.026 (0.041)\tLoss 118.9031 (298.1508)\t\n",
      "begin test\n",
      " * MAE 233.536 \n",
      " * best MAE 233.536 \n",
      "epoch 3, processed 5784 samples, lr 0.0000001000\n",
      "Epoch: [3][0/1928]\tTime 0.212 (0.212)\tData 0.025 (0.025)\tLoss 141.7271 (141.7271)\t\n",
      "Epoch: [3][30/1928]\tTime 0.985 (1.446)\tData 0.045 (0.041)\tLoss 128.8209 (206.4163)\t\n",
      "Epoch: [3][60/1928]\tTime 2.000 (1.402)\tData 0.048 (0.042)\tLoss 18.3737 (266.6182)\t\n",
      "Epoch: [3][90/1928]\tTime 1.088 (1.425)\tData 0.061 (0.042)\tLoss 160.2893 (252.2751)\t\n",
      "Epoch: [3][120/1928]\tTime 0.999 (1.411)\tData 0.025 (0.042)\tLoss 50.1645 (262.6655)\t\n",
      "Epoch: [3][150/1928]\tTime 0.726 (1.388)\tData 0.048 (0.041)\tLoss 30.1530 (276.6046)\t\n",
      "Epoch: [3][180/1928]\tTime 1.468 (1.366)\tData 0.062 (0.041)\tLoss 126.6227 (286.1938)\t\n",
      "Epoch: [3][210/1928]\tTime 0.748 (1.337)\tData 0.049 (0.040)\tLoss 491.9554 (301.6195)\t\n",
      "Epoch: [3][240/1928]\tTime 1.272 (1.362)\tData 0.048 (0.040)\tLoss 220.0283 (282.0677)\t\n",
      "Epoch: [3][270/1928]\tTime 0.783 (1.334)\tData 0.021 (0.040)\tLoss 67.8766 (280.7268)\t\n",
      "Epoch: [3][300/1928]\tTime 1.487 (1.357)\tData 0.053 (0.040)\tLoss 366.5965 (268.9092)\t\n",
      "Epoch: [3][330/1928]\tTime 1.146 (1.341)\tData 0.049 (0.040)\tLoss 95.9334 (260.3325)\t\n",
      "Epoch: [3][360/1928]\tTime 1.164 (1.342)\tData 0.050 (0.040)\tLoss 117.0076 (257.4740)\t\n",
      "Epoch: [3][390/1928]\tTime 1.847 (1.351)\tData 0.020 (0.040)\tLoss 81.1795 (260.4745)\t\n",
      "Epoch: [3][420/1928]\tTime 1.223 (1.325)\tData 0.046 (0.040)\tLoss 335.0778 (287.0104)\t\n",
      "Epoch: [3][450/1928]\tTime 1.949 (1.315)\tData 0.046 (0.039)\tLoss 122.0920 (281.5375)\t\n",
      "Epoch: [3][480/1928]\tTime 1.177 (1.302)\tData 0.054 (0.039)\tLoss 499.4154 (281.6518)\t\n",
      "Epoch: [3][510/1928]\tTime 1.490 (1.310)\tData 0.048 (0.039)\tLoss 159.5705 (272.6620)\t\n",
      "Epoch: [3][540/1928]\tTime 0.916 (1.291)\tData 0.022 (0.038)\tLoss 120.3916 (274.0751)\t\n",
      "Epoch: [3][570/1928]\tTime 0.621 (1.306)\tData 0.047 (0.039)\tLoss 591.2020 (267.9644)\t\n",
      "Epoch: [3][600/1928]\tTime 1.652 (1.313)\tData 0.049 (0.039)\tLoss 630.0780 (265.6465)\t\n",
      "Epoch: [3][630/1928]\tTime 1.387 (1.308)\tData 0.080 (0.039)\tLoss 4551.0640 (270.0380)\t\n",
      "Epoch: [3][660/1928]\tTime 1.547 (1.317)\tData 0.057 (0.039)\tLoss 2129.6880 (273.2637)\t\n",
      "Epoch: [3][690/1928]\tTime 1.158 (1.307)\tData 0.015 (0.039)\tLoss 50.9053 (272.7164)\t\n",
      "Epoch: [3][720/1928]\tTime 0.847 (1.313)\tData 0.052 (0.039)\tLoss 395.0674 (268.7161)\t\n",
      "Epoch: [3][750/1928]\tTime 2.395 (1.315)\tData 0.056 (0.039)\tLoss 26.6261 (269.7855)\t\n",
      "Epoch: [3][780/1928]\tTime 1.065 (1.315)\tData 0.048 (0.039)\tLoss 142.2754 (274.4593)\t\n",
      "Epoch: [3][810/1928]\tTime 1.090 (1.318)\tData 0.043 (0.039)\tLoss 89.3450 (274.2631)\t\n",
      "Epoch: [3][840/1928]\tTime 1.291 (1.319)\tData 0.045 (0.039)\tLoss 105.3254 (283.7413)\t\n",
      "Epoch: [3][870/1928]\tTime 1.110 (1.327)\tData 0.048 (0.039)\tLoss 411.9971 (278.9427)\t\n",
      "Epoch: [3][900/1928]\tTime 2.152 (1.325)\tData 0.049 (0.039)\tLoss 310.6812 (281.3997)\t\n",
      "Epoch: [3][930/1928]\tTime 1.101 (1.328)\tData 0.017 (0.039)\tLoss 23.5441 (280.1202)\t\n",
      "Epoch: [3][960/1928]\tTime 1.736 (1.328)\tData 0.016 (0.040)\tLoss 74.9942 (280.7297)\t\n",
      "Epoch: [3][990/1928]\tTime 1.462 (1.323)\tData 0.050 (0.040)\tLoss 31.2674 (279.4621)\t\n",
      "Epoch: [3][1020/1928]\tTime 2.094 (1.316)\tData 0.057 (0.039)\tLoss 79.9743 (276.9458)\t\n",
      "Epoch: [3][1050/1928]\tTime 0.752 (1.320)\tData 0.051 (0.039)\tLoss 285.3405 (274.6741)\t\n",
      "Epoch: [3][1080/1928]\tTime 0.844 (1.328)\tData 0.053 (0.040)\tLoss 115.4118 (271.7641)\t\n",
      "Epoch: [3][1110/1928]\tTime 1.899 (1.330)\tData 0.041 (0.040)\tLoss 21.4883 (267.6580)\t\n",
      "Epoch: [3][1140/1928]\tTime 1.167 (1.328)\tData 0.027 (0.040)\tLoss 20.9331 (268.1076)\t\n",
      "Epoch: [3][1170/1928]\tTime 1.485 (1.335)\tData 0.050 (0.040)\tLoss 121.1297 (265.4785)\t\n",
      "Epoch: [3][1200/1928]\tTime 1.125 (1.332)\tData 0.020 (0.040)\tLoss 543.8634 (265.1612)\t\n",
      "Epoch: [3][1230/1928]\tTime 0.777 (1.334)\tData 0.025 (0.040)\tLoss 39.6351 (266.5705)\t\n",
      "Epoch: [3][1260/1928]\tTime 2.074 (1.340)\tData 0.048 (0.040)\tLoss 87.8824 (266.3987)\t\n",
      "Epoch: [3][1290/1928]\tTime 0.746 (1.338)\tData 0.013 (0.040)\tLoss 233.8290 (268.3879)\t\n",
      "Epoch: [3][1320/1928]\tTime 0.889 (1.342)\tData 0.018 (0.040)\tLoss 31.5782 (268.2958)\t\n",
      "Epoch: [3][1350/1928]\tTime 2.131 (1.339)\tData 0.058 (0.040)\tLoss 76.3417 (265.2522)\t\n",
      "Epoch: [3][1380/1928]\tTime 1.066 (1.344)\tData 0.050 (0.040)\tLoss 172.4333 (270.5251)\t\n",
      "Epoch: [3][1410/1928]\tTime 0.886 (1.344)\tData 0.042 (0.040)\tLoss 6.8268 (274.1107)\t\n",
      "Epoch: [3][1440/1928]\tTime 0.911 (1.349)\tData 0.018 (0.040)\tLoss 24.8735 (276.0227)\t\n",
      "Epoch: [3][1470/1928]\tTime 1.860 (1.348)\tData 0.021 (0.040)\tLoss 457.1123 (276.8005)\t\n",
      "Epoch: [3][1500/1928]\tTime 1.094 (1.342)\tData 0.018 (0.040)\tLoss 228.8813 (278.5274)\t\n",
      "Epoch: [3][1530/1928]\tTime 1.135 (1.346)\tData 0.027 (0.040)\tLoss 124.5576 (280.8646)\t\n",
      "Epoch: [3][1560/1928]\tTime 1.070 (1.341)\tData 0.047 (0.040)\tLoss 74.3128 (283.3533)\t\n",
      "Epoch: [3][1590/1928]\tTime 1.174 (1.345)\tData 0.060 (0.040)\tLoss 54.6373 (284.5043)\t\n",
      "Epoch: [3][1620/1928]\tTime 1.141 (1.345)\tData 0.027 (0.040)\tLoss 174.0170 (285.1777)\t\n",
      "Epoch: [3][1650/1928]\tTime 0.719 (1.343)\tData 0.028 (0.040)\tLoss 118.4163 (285.4949)\t\n",
      "Epoch: [3][1680/1928]\tTime 0.805 (1.345)\tData 0.034 (0.040)\tLoss 66.2550 (285.7634)\t\n",
      "Epoch: [3][1710/1928]\tTime 1.561 (1.344)\tData 0.071 (0.040)\tLoss 112.9396 (282.8478)\t\n",
      "Epoch: [3][1740/1928]\tTime 1.331 (1.346)\tData 0.015 (0.040)\tLoss 268.9561 (282.7337)\t\n",
      "Epoch: [3][1770/1928]\tTime 1.406 (1.341)\tData 0.048 (0.040)\tLoss 77.7311 (285.2285)\t\n",
      "Epoch: [3][1800/1928]\tTime 1.352 (1.346)\tData 0.037 (0.040)\tLoss 954.9708 (285.8195)\t\n",
      "Epoch: [3][1830/1928]\tTime 1.951 (1.348)\tData 0.050 (0.040)\tLoss 22.0225 (283.9270)\t\n",
      "Epoch: [3][1860/1928]\tTime 0.768 (1.345)\tData 0.052 (0.040)\tLoss 10.8991 (281.9217)\t\n",
      "Epoch: [3][1890/1928]\tTime 1.530 (1.350)\tData 0.053 (0.040)\tLoss 791.9428 (282.1702)\t\n",
      "Epoch: [3][1920/1928]\tTime 1.441 (1.348)\tData 0.031 (0.040)\tLoss 39.7012 (288.1400)\t\n",
      "begin test\n",
      " * MAE 292.954 \n",
      " * best MAE 233.536 \n"
     ]
    }
   ],
   "source": [
    "training('part_A_train.json', 'part_A_val.json','0', '0', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
